{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron and back propagation\n",
    "\n",
    "## Some stuffs to begin with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot(size):\n",
    "    limit = np.sqrt(6.0 / sum(size))\n",
    "    return np.random.uniform(-limit, limit, size)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_prime(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return x * (x >= 0)\n",
    "\n",
    "\n",
    "def relu_prime(y):\n",
    "    return 1 * (y >= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "let $lr$ a constant in [0, 1]\n",
    "\n",
    "let $x$, $y$ 2 vectors, where:\n",
    "- $x$ is the input of dimension $(k, n)$\n",
    "- $y$ is the expected output of dimension $(k, m)$\n",
    "\n",
    "let $w$, $b$ 2 matrices, where\n",
    "- $w$ is the weights of the layer of dimension $(n, m)$\n",
    "- $b$ the bias of the layer of dimension $(1, m)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "w = glorot((2, 1))\n",
    "b = np.zeros((1, 1))\n",
    "\n",
    "x = np.array(\n",
    "    [\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [1, 0],\n",
    "        [1, 1],\n",
    "    ]\n",
    ")\n",
    "y_true = np.array(\n",
    "    [\n",
    "        [0],\n",
    "        [0],\n",
    "        [0],\n",
    "        [1],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass\n",
    "\n",
    "We will compute the forward and backward pass of a one dense layer with $n$ inputs and $m$ outputs with an activation function of type $relu$.\n",
    "\n",
    "**Compute the linear transformation of $x$**\n",
    "\n",
    "```math\n",
    "\\hat{z} = \\hat{x} w + b\n",
    "```\n",
    "\n",
    "Then give some non linearity by applying the activation function\n",
    "\n",
    "```math\n",
    "\\hat{y} = relu(\\hat{z}), relu: x \\mapsto xH(x)\n",
    "```\n",
    "where H is the Heaviside step function\n",
    "```math\n",
    "H(x) = \\begin{cases}\n",
    "   1 &\\text{if } x >= 0 \\\\\n",
    "   0 &\\text{if } x < 0\n",
    "\\end{cases}\n",
    "```\n",
    "\n",
    "**Compute the $loss$ between the output $\\hat{y}$ and the expected output $y$**\n",
    "\n",
    "```math\n",
    "loss(\\hat{y}, y) = \\frac{1}{2}(y - \\hat{y})^2 \\\\\n",
    "```\n",
    "\n",
    "## Backward pass\n",
    "\n",
    "We want to know how much we need to adjust our weights and biases of the layer to minimize the loss function. Basically we need to substract the gradients of the loss along w and b. It is basically a Newton algorithm to find a minima of the forward function minimizing the loss.\n",
    "\n",
    "**Compute the gradient of the $loss$ along $w$**\n",
    "\n",
    "```math\n",
    "\\tag{a} \\nabla{loss(\\hat{y}, y)} \\ldotp w = \\frac{\\partial{loss(\\hat{y}, y)}}{\\partial{w}} \n",
    "```\n",
    "\n",
    "Using the chain rules, we got\n",
    "\n",
    "```math\n",
    "\\tag{b} \\frac{\\partial{loss(\\hat{y}, y)}}{\\partial{w}} = \\frac{\\partial{loss(\\hat{y}, y)}}{\\partial{\\hat{y}}} \\frac{\\partial{\\hat{y}}}{\\partial{\\hat{z}}} \\frac{\\partial{\\hat{z}}}{\\partial{w}}\n",
    "```\n",
    "\n",
    "where\n",
    "\n",
    "```math\n",
    "\\tag{c}  \\frac{\\partial{loss(\\hat{y}, y)}}{\\partial{\\hat{y}}} = -(y - \\hat{y}) = \\hat{y} - y\n",
    "```\n",
    "```math\n",
    "\\tag{d}  \\frac{\\partial{\\hat{y}}}{\\partial{\\hat{z}}} = \\frac{\\partial{relu(\\hat{z})}}{\\partial{\\hat{z}}} = \\frac{\\partial{(\\hat{z}H(\\hat{z}))}}{\\partial{\\hat{z}}} = 1 * \\begin{cases}\n",
    "   1 &\\text{if } \\hat{z} >= 0 \\\\\n",
    "   0 &\\text{if } \\hat{z} < 0\n",
    "\\end{cases} = H(\\hat{z})\n",
    "```\n",
    "```math\n",
    "\\tag{e}  \\frac{\\partial{(x w + b)}}{\\partial{w}} = x^T\n",
    "```\n",
    "\n",
    "Combining all together:\n",
    "\n",
    "```math\n",
    "\\boxed{\\nabla{loss(\\hat{y}, y)} \\ldotp w =  x^T \\otimes (\\hat{y} - y) H(\\hat{z})}\n",
    "```\n",
    "\n",
    "**Compute the gradient of the $loss$ along $b$**\n",
    "\n",
    "```math\n",
    "\\tag{a} \\nabla{loss(\\hat{y}, y)} \\ldotp b = \\frac{\\partial{loss(\\hat{y}, y)}}{\\partial{b}} \n",
    "```\n",
    "\n",
    "Using the chain rules, we got\n",
    "\n",
    "```math\n",
    "\\tag{b} \\frac{\\partial{loss(\\hat{y}, y)}}{\\partial{b}} = \\frac{\\partial{loss(\\hat{y}, y)}}{\\partial{\\hat{y}}} \\frac{\\partial{\\hat{y}}}{\\partial{\\hat{z}}} \\frac{\\partial{\\hat{z}}}{\\partial{b}}\n",
    "```\n",
    "\n",
    "where\n",
    "\n",
    "```math\n",
    "\\tag{c}  \\frac{\\partial{loss(\\hat{y}, y)}}{\\partial{\\hat{y}}} = -(y - \\hat{y}) = \\hat{y} - y\n",
    "```\n",
    "```math\n",
    "\\tag{d}  \\frac{\\partial{\\hat{y}}}{\\partial{\\hat{z}}} = \\frac{\\partial{relu(\\hat{z})}}{\\partial{\\hat{z}}} = \\frac{\\partial{(\\hat{z}H(\\hat{z}))}}{\\partial{\\hat{z}}} = 1 * \\begin{cases}\n",
    "   1 &\\text{if } \\hat{z} >= 0 \\\\\n",
    "   0 &\\text{if } \\hat{z} < 0\n",
    "\\end{cases} = H(\\hat{z})\n",
    "```\n",
    "```math\n",
    "\\tag{e}  \\frac{\\partial{(x w + b)}}{\\partial{b}} = 1\n",
    "```\n",
    "\n",
    "Combining all together:\n",
    "\n",
    "```math\n",
    "\\boxed{\\nabla{loss(\\hat{y}, y)} \\ldotp b = (\\hat{y} - y) H(\\hat{z})}\n",
    "```\n",
    "\n",
    "**Update the w and b matrices with the respective gradients of the loss.**\n",
    "\n",
    "```math\n",
    "\\boxed{\n",
    "   \\begin{aligned}\n",
    "   w = w - lr * \\nabla{loss(\\hat{y}, y)} \\ldotp w \\\\\n",
    "   b = b - lr * \\nabla{loss(\\hat{y}, y)} \\ldotp b\n",
    "   \\end{aligned}\n",
    "}\n",
    "```\n",
    "\n",
    "Now, we need to pass the gradient of the loss to the next layer of the neural network and then perform the same calculation as above.\n",
    "\n",
    "**Compute the gradient of the $loss$ along $\\hat{x}$**\n",
    "\n",
    "```math\n",
    "\\tag{a} \\nabla{loss(\\hat{y}, y)} \\ldotp \\hat{x} = \\frac{\\partial{loss(\\hat{y}, y)}}{\\partial{\\hat{x}}} \n",
    "```\n",
    "\n",
    "Using the chain rules, we got\n",
    "\n",
    "```math\n",
    "\\tag{b} \\frac{\\partial{loss(\\hat{y}, y)}}{\\partial{\\hat{x}}} = \\frac{\\partial{loss(\\hat{y}, y)}}{\\partial{\\hat{y}}} \\frac{\\partial{\\hat{y}}}{\\partial{\\hat{z}}} \\frac{\\partial{\\hat{z}}}{\\partial{\\hat{x}}}\n",
    "```\n",
    "\n",
    "where\n",
    "\n",
    "```math\n",
    "\\tag{c}  \\frac{\\partial{loss(\\hat{y}, y)}}{\\partial{\\hat{y}}} = -(y - \\hat{y}) = \\hat{y} - y\n",
    "```\n",
    "```math\n",
    "\\tag{d}  \\frac{\\partial{\\hat{y}}}{\\partial{\\hat{z}}} = \\frac{\\partial{relu(\\hat{z})}}{\\partial{\\hat{z}}} = \\frac{\\partial{(\\hat{z}H(\\hat{z}))}}{\\partial{\\hat{z}}} = 1 * \\begin{cases}\n",
    "   1 &\\text{if } \\hat{z} >= 0 \\\\\n",
    "   0 &\\text{if } \\hat{z} < 0\n",
    "\\end{cases} = H(\\hat{z})\n",
    "```\n",
    "```math\n",
    "\\tag{e}  \\frac{\\partial{(x w + b)}}{\\partial{x}} = w^T\n",
    "```\n",
    "\n",
    "Combining all together:\n",
    "\n",
    "```math\n",
    "\\boxed{\\nabla{loss(\\hat{y}, y)} \\ldotp \\hat{x} = (\\hat{y} - y) H(\\hat{z}) \\otimes w^T}\n",
    "```\n",
    "\n",
    "**Finally we can pass the gradient of the loss to the next layer (Hence gradient descent).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(5000):\n",
    "    # Forward\n",
    "    y_pred = relu(x @ w + b)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = 0.5 * (y_true - y_pred) ** 2\n",
    "\n",
    "    # Compute loss gradient along y_pred\n",
    "    grad_loss = y_pred - y_true\n",
    "\n",
    "    # Compute loss gradient along the function relu\n",
    "    grad_loss = grad_loss * relu_prime(y_pred)\n",
    "\n",
    "    # Compute the loss gradient along the weights and biases\n",
    "    grad_w = x.T @ grad_loss\n",
    "    grad_b = np.mean(grad_loss)\n",
    "\n",
    "    # Compute the loss gradient along x, ready to pass the loss gradient to the next layer\n",
    "    grad_loss = grad_loss @ w.T\n",
    "\n",
    "    # Adjust weigts and biases\n",
    "    w -= grad_w * lr\n",
    "    b -= grad_b * lr\n",
    "\n",
    "np.all(np.round(y_pred) == y_true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel-py311",
   "language": "python",
   "name": "kernel-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
