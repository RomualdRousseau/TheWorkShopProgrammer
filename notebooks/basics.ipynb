{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Natural Language Processing\n",
    "\n",
    "The basics steps:\n",
    "\n",
    "- Pattern recognition\n",
    "- Cleansing\n",
    "- Tokenize\n",
    "- Lemmenize\n",
    "- Statitics\n",
    "- Remove stop words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import some python stuffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need a little bit of data.\n",
    "\n",
    "Format:\n",
    "list of tuple (document, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    (\"I am happy\", \"happy\"),\n",
    "    (\"I am not sad\", \"happy\"),\n",
    "    (\"I am sad\", \"sad\"),\n",
    "    (\"I am not happy\", \"sad\"),\n",
    "    (\"John Doe is good\", \"happy\"),\n",
    "    (\"John John Doe is bad\", \"sad\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_first(fn, list_tuples):\n",
    "    return [(fn(x[0]), x[1]) for x in list_tuples]\n",
    "\n",
    "\n",
    "def map_second(fn, list_tuples):\n",
    "    return [(x[0], fn(x[1])) for x in list_tuples]\n",
    "\n",
    "\n",
    "def map_apply(fn):\n",
    "    return lambda x: list(map(fn, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "\n",
    "entitites = {\"__person\": r\"\\b[A-Z]\\w+(\\s+[A-Z]\\w+)*\\b\"}\n",
    "\n",
    "variables = {}\n",
    "\n",
    "\n",
    "def simple_cleanser(doc):\n",
    "    global n\n",
    "    for k, v in entitites.items():\n",
    "        completed = False\n",
    "        while not completed:\n",
    "            match = re.search(v, doc)\n",
    "            completed = match is None\n",
    "            if not completed:\n",
    "                var_name, var_value = k + str(n), match.group()\n",
    "                n += 1\n",
    "\n",
    "                variables[var_name] = var_value\n",
    "\n",
    "                s, e = match.span()\n",
    "                doc = doc[:s] + var_name + doc[e:]\n",
    "        return doc\n",
    "\n",
    "\n",
    "var_documents = map_first(simple_cleanser, documents)\n",
    "var_documents, variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleansing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_cleanser(doc):\n",
    "    return doc.lower()\n",
    "\n",
    "\n",
    "cleansed_documents = map_first(simple_cleanser, var_documents)\n",
    "cleansed_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(doc):\n",
    "    return doc.split(\" \")\n",
    "\n",
    "\n",
    "tokenized_documents = map_first(simple_tokenizer, cleansed_documents)\n",
    "tokenized_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = {\n",
    "    \"be\": r\"be|am|are|is\",\n",
    "    \"happy\": r\"happ(y|iest)\",\n",
    "    \"sad\": r\"sad(est)?\",\n",
    "}\n",
    "\n",
    "\n",
    "def simple_lemmenizer(token):\n",
    "    return next(filter(lambda x: re.search(lexicon[x], token) is not None, lexicon), token)\n",
    "\n",
    "\n",
    "lemmenized_documents = map_first(map_apply(simple_lemmenizer), tokenized_documents)\n",
    "lemmenized_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = sorted({token for tokens, _ in lemmenized_documents for token in tokens})\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute term frequency for each document (tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency_in_doc(tokens):\n",
    "    sum_t = len(tokens)\n",
    "    t_f = [(token, 1) for token in tokens]\n",
    "    return {word: sum([f for t, f in t_f if t == word]) / sum_t for word in vocabulary}\n",
    "\n",
    "\n",
    "term_frequencies = [term_frequency_in_doc(tokens) for tokens, _ in lemmenized_documents]\n",
    "term_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute inverse document frequency (idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_document_frequency(word):\n",
    "    sum_d = len(lemmenized_documents)\n",
    "    t_f = [(word, 1 if word in tokens else 0) for word in vocabulary for tokens, _ in lemmenized_documents]\n",
    "    n_t = sum([f for t, f in t_f if t == word])\n",
    "    return -math.log(n_t / sum_d)\n",
    "\n",
    "\n",
    "inverse_document_frequencies = {word: inverse_document_frequency(word) for word in vocabulary}\n",
    "inverse_document_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(doc, token):\n",
    "    return term_frequencies[doc][token] * inverse_document_frequencies[token]\n",
    "\n",
    "\n",
    "tf_idfs = [\n",
    "    ([(token, tf_idf(doc, token)) for token in tokens], label)\n",
    "    for doc, (tokens, label) in enumerate(lemmenized_documents)\n",
    "]\n",
    "tf_idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_documents = [([token for token, tf_idf in tokens if tf_idf > 0], label) for tokens, label in tf_idfs]\n",
    "final_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [sum([tf_idf for _, tf_idf in tokens if tf_idf > 0]) for tokens, _ in tf_idfs]\n",
    "y = [sum([tf_idf for _, tf_idf in tokens if tf_idf > 0]) for tokens, _ in tf_idfs]\n",
    "n = [label for _, label in tf_idfs]\n",
    "\n",
    "plt.scatter(x, y)\n",
    "for x_, y_, n_ in zip(x, y, n):\n",
    "    plt.annotate(n_, (x_, y_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
